<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>abar-cs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="abar-cs_files/libs/clipboard/clipboard.min.js"></script>
<script src="abar-cs_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="abar-cs_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="abar-cs_files/libs/quarto-html/popper.min.js"></script>
<script src="abar-cs_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="abar-cs_files/libs/quarto-html/anchor.min.js"></script>
<link href="abar-cs_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="abar-cs_files/libs/quarto-html/quarto-syntax-highlighting-e4fbaa5b9575d7f6de7c906568c40117.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="abar-cs_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="abar-cs_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="abar-cs_files/libs/bootstrap/bootstrap-62397465345a1d089ccac9fcec976ec5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
</head><body class="fullcontent quarto-light">\usepackage[none]{hyphenat}
% Set default font size for body text to 12pt
\AtBeginDocument{\pagenumbering{gobble}\fontsize{12}{16}\selectfont}
% Set all headings to Times New Roman
\usepackage{sectsty}
\allsectionsfont{\rmfamily}
% Custom command for main section headings (define only if not already defined)
\providecommand{\mainsection}[1]{\begin{center}{\rmfamily\textbf{\fontsize{14}{20}\selectfont #1}}\end{center}}
% Custom command for ToC lines with dot leaders
\newcommand{\tocline}[1]{\noindent\makebox[\textwidth][l]{#1}}
% Header and footer (set only once)
\usepackage{fancyhdr}
\usepackage{xcolor}
\fancypagestyle{main}{
  \fancyhf{}
  \fancyhead[L]{\textit{\textcolor{gray}{RADAR Signal Processing Using Compressed Sensing}}}
  \fancyfoot[R]{\textit{\textcolor{gray}{\thepage}}}
  \fancyfoot[L]{\textit{\textcolor{gray}{School Of Engineering, CUSAT}}}
}
\pagestyle{main}

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">
\begin{titlepage}
\centering
\vspace*{2cm}
% \includegraphics[width=0.2\textwidth]{your-logo.png}\par
\vspace{1cm}
{\fontfamily{ptm}\selectfont
{\Huge\bfseries RADAR SIGNAL PROCESSING USING COMPRESSED SENSING \par}
\vspace{0.5cm}
{\Large Internship Report \par}
\vspace{2cm}
{\large\bfseries AUTHORS\par}
{\large\itshape Abhinav M Balakrishnan\par}
{\large\itshape Arun Ramesh\par}
\vspace{2cm}
{\large\bfseries INSTITUTE\par}
{\large\itshape School of Engineering, CUSAT\par}
\vspace{2cm}
}
\vfill
\end{titlepage}
\clearpage




<!--
For each main section, use the following format:
-->
<p>This acknowledgment is a testament to the intensive drive and technical competence of many individuals who have contributed to the success of our project.</p>
<p>Special thanks to Shri B.S. Teza, Scientist ‘E’, ASL DRDO, for not only selecting us for this internship, but also for his consistent guidance, encouragement, and valuable insights throughout the course of our project.</p>
<p>We also thank our professors for their constant support and inspiration. Special thanks to our HoD, Dr.&nbsp;Deepa Shankar and our class co-ordinator and department faculty, Dr.&nbsp;Mridula S.</p>
<!--
ABSTRACT
-->
<p>This project explores the application of compressed sensing techniques to radar signal processing, aiming to overcome the limitations imposed by traditional Nyquist sampling. It also presents the theoretical foundations of compressed sensing, details key reconstruction algorithms such as Orthogonal Matching Pursuit (OMP), Iterative Shrinkage Thresholding Algorithm (ISTA), and Coordinate Descent (CoD), and evaluates their performance through simulations and Monte Carlo trials. The results demonstrate the effectiveness and trade-offs of these algorithms in both noiseless and noisy environments, highlighting their potential for efficient and robust radar signal acquisition and processing. The project also explores the future possibilities and modifications for these algorithms for executing real time RADAR processing.</p>
<p>The GitHub repository for all the code and results can be accessed: <a href="https://github.com/ninja-boy/CS-Algorithms">CS Algorithms</a></p>
<!--
Example for CHAPTERS:
-->
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1.1 INTRODUCTION</h2>
<p>The Nyquist–Shannon sampling theorem is a theorem in the field of signal processing which serves as a fundamental bridge between continuous-time signals and discrete-time signals. It establishes a sufficient condition for a sample rate that permits a discrete sequence of samples to capture all the information from a continuous-time signal of finite bandwidth.</p>
<p>For a signal of frequency <span class="math inline">\(f_\text{signal}\)</span>, the minimum sampling rate required to avoid aliasing, according to the Nyquist criterion is,</p>
<p><span class="math display">\[\begin{equation}
\boxed{
  \begin{array}{c}
    \textbf{\large Nyquist-Shannon Sampling Criteria} \\[1.5ex]
    f_\mathrm{s} \geq 2f_\mathrm{signal} \\[1.5ex]
    \end{array}
    }
\end{equation}\]</span></p>
<p>This means that the sampling frequency must be at least twice the highest frequency present in the signal to ensure perfect reconstruction from its samples.</p>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">1.2 LIMITATIONS</h2>
<p>One of the main limitations of the Nyquist sampling theorem is the requirement for high sampling rates when dealing with signals that contain high-frequency components, which can be challenging to achieve in practice due to several reasons:</p>
<ul>
<li><strong>Hardware Limitations:</strong> Analog-to-digital converters (ADCs), capable of very high sampling rates are expensive and may not be readily available. The speed and resolution of ADCs are often limited by current technology.</li>
<li><strong>Data Storage and Processing:</strong> High sampling rates generate large volumes of data, which require significant storage capacity and processing power. This can make real-time processing and analysis difficult or costly.</li>
<li><strong>Power Consumption:</strong> Systems operating at high sampling rates typically consume more power, which is a critical concern in portable or battery-powered devices.</li>
<li><strong>Noise Sensitivity:</strong> At higher frequencies, electronic components are more susceptible to noise and interference, which can degrade the quality of the sampled signal.</li>
</ul>
<p>These limitations motivate the development of alternative sampling techniques, such as <strong>Compressed Sensing</strong>, which aim to reconstruct signals accurately from fewer samples than required by the traditional Nyquist criterion, especially when the signal is sparse or compressible in some domain.</p>
</section>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">2.1 INTRODUCTION</h2>
<p>The limitations of the Nyquist criterion, especially in applications requiring high data rates or operating under hardware constraints, have led to the exploration of new signal acquisition paradigms. Compressed Sensing (CS) is one such approach that leverages the sparsity of signals in some domain to enable accurate reconstruction from far fewer samples than traditionally required.</p>
</section>
<section id="motivations-for-compressed-sensing" class="level2">
<h2 class="anchored" data-anchor-id="motivations-for-compressed-sensing">2.2 MOTIVATIONS FOR COMPRESSED SENSING</h2>
<p>Key motivations for using compressed sensing include:</p>
<ul>
<li><strong>Efficient Data Acquisition:</strong> CS allows for the collection of only the most informative measurements, reducing the burden on data acquisition systems.</li>
<li><strong>Reduced Storage and Transmission Costs:</strong> By acquiring fewer samples, CS minimizes the amount of data that needs to be stored or transmitted, which is particularly beneficial in bandwidth-limited or remote sensing scenarios.</li>
<li><strong>Lower Power Consumption:</strong> Fewer samples mean less processing and lower power requirements, which is advantageous for battery-powered and embedded systems.</li>
<li><strong>Enabling New Applications:</strong> CS opens up possibilities for applications where traditional sampling is impractical, such as medical imaging, wireless communications, and radar signal processing.</li>
</ul>
<p>In the following chapters, we explore the principles of compressed sensing and its application to radar signal processing.</p>
</section>
<section id="fundamental-terms" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-terms">2.3 FUNDAMENTAL TERMS</h2>
<p>Before delving deeper into compressed sensing, it is important to understand some fundamental terms:</p>
<ul>
<li><strong>Sparsity:</strong> A signal is said to be sparse if most of its coefficients are zero or close to zero. Sparsity is a key assumption in compressed sensing.</li>
<li><strong>Basis:</strong> In compressed sensing, a basis is a set of vectors (such as Fourier, wavelet, or DCT bases) in which the signal can be represented as a linear combination. The choice of basis is crucial, as it determines the sparsity and thus the effectiveness of compressed sensing for a given signal. It is also called the <strong>dictionary matrix</strong>.</li>
<li><strong>Measurement Matrix:</strong> In compressed sensing, the measurement matrix is used to acquire linear projections of the original signal. It is also known as the dictionary matrix or sampling matrix.</li>
<li><strong>Reconstruction Algorithm:</strong> Algorithms such as Basis Pursuit, Orthogonal Matching Pursuit (OMP), and LASSO are used to recover the original sparse signal from the compressed measurements.</li>
</ul>
<p>Understanding these terms is essential for grasping the principles and practical implementation of compressed sensing.</p>
</section>
<section id="mathematical-model" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-model">2.4 MATHEMATICAL MODEL</h2>
<p>In compressed sensing, the measurement process can be mathematically modeled as:</p>
<p><span class="math display">\[\begin{equation}
  \mathbf{y} = \phi \mathbf{x}
\end{equation}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is the <strong>original signal</strong> (which is assumed to be sparse or compressible in some basis)</li>
<li><span class="math inline">\(\phi \in \mathbb{R}^{m \times n}\)</span> is the <strong>measurement matrix</strong> (with <span class="math inline">\(m &lt; n\)</span>)</li>
<li><span class="math inline">\(\mathbf{y} \in \mathbb{R}^m\)</span> is the <strong>compressed (measurement) vector</strong>.</li>
</ul>
<p>If the signal <span class="math inline">\(\mathbf{x}\)</span> is not sparse in its original domain but is sparse in some transform domain (e.g., DCT, DFT, or wavelet), we can write <span class="math inline">\(\mathbf{x} = \Psi \mathbf{s}\)</span>, where <span class="math inline">\(\Psi\)</span> is the <strong>basis matrix</strong> and <span class="math inline">\(\mathbf{s}\)</span> is the <strong>sparse coefficient vector</strong>. The measurement model then becomes:</p>
<p><span class="math display">\[\begin{equation}
  \mathbf{y} = \phi \Psi \mathbf{s} = \Theta \mathbf{s}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\Theta = \phi \Psi\)</span> is the <strong>sensing matrix</strong>.</p>
<p>The goal of compressed sensing is to recover <span class="math inline">\(\mathbf{x}\)</span> (or <span class="math inline">\(\mathbf{s}\)</span>) from the measurements <span class="math inline">\(\mathbf{y}\)</span>, given knowledge of <span class="math inline">\(\phi\)</span> (and <span class="math inline">\(\Psi\)</span> if applicable), by exploiting the sparsity of the signal.</p>
<p>The various algorithms are used for reconstructing back the original signal that was initially compressed by the process as shown previously.</p>
</section>
<section id="orthogonal-matching-pursuit-omp" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-matching-pursuit-omp">3.1 ORTHOGONAL MATCHING PURSUIT (OMP)</h2>
<p>The OMP algorithm is an iterative greedy algorithm used to recover sparse signals from compressed measurements. At each iteration, it selects the column of the measurement matrix that is most correlated with the current residual and updates the solution accordingly. The process continues until a sufficiently small residual is met. The steps are listed below, as shown below</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="C:\Users\NinjaBoyASUS\OneDrive\Dokumen\Abhi\Internship\OMP-Python\omp_algorithm.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm<span class="citation" data-cites="omp-intro">[@omp-intro]</span></figcaption>
</figure>
</div>
<p>This algorithm can be implemented in MATLAB and Python with necessary toolboxes and libraries.</p>
</section>
<section id="algorithm-implementation" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-implementation">3.1.1 Algorithm Implementation</h2>
<ul>
<li><strong>MATLAB</strong></li>
</ul>
<p>Here, The code is divided into two main parts: the OMP function definition and the signal generation/reconstruction workflow. The omp function implements the OMP algorithm. It takes as input a measurement matrix A, a measurement vector b, and the sparsity level K. The function normalizes the columns of A and initializes the residual r as the measurement vector. It iteratively selects the column of A most correlated with the current residual, adds its index to the support set Lambda, and solves a least-squares problem to update the estimated sparse vector x. The residual is updated accordingly. This process repeats for K iterations, corresponding to the assumed sparsity of the original signal. For ideal omp implementation, n,m,k are defined and a random gaussian sensing matrix is created. then a sparse signal is created in frequency domain and is multiplied with the sensing matrix to create the measurements(output). Now, the omp algorithm is used to recover the frequency domain signal back and the rreconstructed and original signals are plotted together to check for errors. For noisy implementation, an additional gaussian noise is generated and added to the output measurement vector and this noisy output is given to the omp algorithm. Then a sum of random time domain sinusoidal input was given with <span class="math inline">\(\Psi\)</span> as a random gaussian matrix and <span class="math inline">\(\phi\)</span> as a inbuilt function for dft, dftmtx(). dft was taken instead of idft since the omp was calculated in the frequency domain. the ouput was then converted to time domain using ifft() and was plotted.</p>
<ul>
<li><p><strong>Python</strong></p>
<p>Libraries like <strong>numpy</strong> and <strong>matplotlib</strong> are imported for mathematical operations and plotting results respectively.</p>
<ul>
<li><p><strong>Stage 1:</strong> The basic implementation was done by taking length of signal (n), number of measurements (m) and non-zero values or sparsity (k) as input. The sensing matrix was assumed to be filled with random gaussian values.</p></li>
<li><p><strong>Stage 2:</strong> The next stage involved taking a sum of three sinusoidal signals as input signal (k = 3) and it is converted to a more sparser domain with <strong>Discrete Cosine Transform (DCT)</strong>. The function is used by importing the <strong>scipy</strong> library. While initially k was fixed, it is then taken as an input from user. DCT was initially tested for a single sine wave as well as for sum of sine waves of different frequencies, as shown in the figure below.</p></li>
<li><p><strong>Stage 3:</strong> In the above stages, reconstruction was observed for pure signals. So, a noise (in dB) was introduced before the reconstruction process.</p></li>
</ul></li>
</ul>
<p>All these stages were plotted and the error was calculated and observed.</p>
</section>
<section id="monte-carlo-trials" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-trials">3.1.2 Monte-Carlo Trials</h2>
<p>Monte Carlo trials are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The is used to understand the behaviour of the algorithm under change in parameters, including sparsity, noise and number of measurements taken. It is useful for analysis, understanding its performance for various values of multiple inputs. In other words, this acts like a testbench for the algorithm.</p>
<p>The input values were stored in a list and these were fed to the trial algorithm. The error was calculated for a number of trials for the same input values, and only the average error is plotted to prevent unwanted variations in reconstruction.</p>
</section>
<section id="observations-results" class="level2">
<h2 class="anchored" data-anchor-id="observations-results">3.1.3 Observations &amp; Results</h2>
<ul>
<li><strong>MATLAB</strong></li>
</ul>
<p>The algorithm was initially tested directly in frequncy domain. In its ideal form(ie. without noise), for a low enough sparsity, the algorithm perfectly reconstructed the frequency and the amplitude values of compressed signal. Then, two values of noise was given(SNR=0dB and SNR=20dB). The Algorithm was able to reconstruct the signal near-perfectly for an SNR of 20 dB. For an SNR of 0 dB(signal power=noise power), the results were more inaccurate, both in terms of position on the graph(frequency) and the amplitude values.The algorithm was able to reconstruct some parts of the signal with a fair amount of accuracy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP_MATLAB\img\output\omp_ideal\omp_ideal_1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Signal Reconstruction:Ideal (No noise added)</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP_MATLAB\img\output\omp_noisy\omp_noisy1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Signal Reconstruction: 20dB</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP_MATLAB\img\output\omp_noisy\0db\omp_0db1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Signal Reconstruction: 0dB</figcaption>
</figure>
</div>
<p>Next, the code was used to implement sinusoids in time domain. A sum of 5 real sinusoids was given as input to the algorithm. Then the output is plotted along with the original signal to compare them. Initially, the algorithm gave an output which had its amplitude greatly decreased w.r.t the original signal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP_MATLAB\img\error\omp_sine\omp_err_1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Signal Reconstruction: 0dB</figcaption>
</figure>
</div>
<p>Further testing of the code showed the importance of normalising the theta matrix. The normalised theta matrix showed the correct output. After this was resolved, the algorithm was able to reconstruct the signal fairly accurately. Smaller peaks of the input signal was harder to reconstruct for the algorithm, and also, there was a reduction in the amplitude of the reconstructed signal w.r.t the original signal.A slight phase shift was observed in some outputs when the code is run for different random inputs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP_MATLAB\img\output\omp_sine\omp_sine1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Signal Reconstruction: sinusoidal input</figcaption>
</figure>
</div>
<ul>
<li><strong>Python</strong></li>
</ul>
<p>For <strong>Stage 1</strong> implementation, the sparse matrix is already created by specifying k. So, the compressed matrix (y) is generated by just multiplying sensing matrix (<span class="math inline">\(\Theta\)</span>) and the generated sparse matrix (s). The results are plotted as shown below,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/True-Recon/omp-alg-256.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm Stage 1 Implementation: Perfect Reconstruction</figcaption>
</figure>
</div>
<p>While the reconstruction as shown above is very accurate, it is not always the case. As sparsity increases, the measurements to be taken also increases. Hence, there are some necessary conditions for perfect recovery of a signal. As mentioned in <span class="citation" data-cites="rani-cs">[@rani-cs]</span>, the relation between n, m and k is:-</p>
<p><span class="math display">\[\begin{equation}
\boxed{
    m \geq C \cdot k \cdot \log\left(\frac{n}{k}\right)
}
\end{equation}\]</span></p>
<p>where <strong>C</strong> is a constant almost equal to 2.</p>
<p>Hence, if the above equation is not satisfied, then reconstruction is very difficult. The failed reconstruction is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/Error/omp-alg-256.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm Stage 1 Implementation: Failed Reconstruction</figcaption>
</figure>
</div>
<p>When it comes to <strong>Stage 2</strong> implementation, the sensing matrix is divided into a basis matrix and measurement matrix. The basis matrix is used to convert our input signal to a sparser signal. For sinusoidal inputs, it is best to represent the signals in its frequency domain. So, FFT or DCT can be used. Since, all sinusoids are real signals, DCT was possible. The sum of sinusoids were converted to DCT and the results are being plotted to check its sparsity.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/sine-dct.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm Stage 2 Implementation: DCT Basis on Sinusoidal signals</figcaption>
</figure>
</div>
<p>The sinusoidal signal is initially tested for various values of n and m, keeping k = 3. Some of the results are plotted as shown,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/omp-sine-n128-m60.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm Stage 2 Implementation: For n = 128, m = 60</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/omp-sine-n128-m100.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP Algorithm Stage 2 Implementation: For n = 128, m = 100</figcaption>
</figure>
</div>
<p>So, generally we can say as <strong>number of measurements increases, the reconstruction error decreases</strong>. Till now, no noise has been considered during the reconstruction. To analyse the algorithm for each value of n, m, k and even noise, it is difficult for us to understand the trend of error. So, a Monte Carlo trial has been implemented on the OMP algorithm for three variable parameters, <strong>measurements</strong>, <strong>sparsity</strong> and <strong>noise</strong>. So, all three parameters are compared and the results are plotted.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/omp-montecarlo-sum.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Monte Carlo Trial: Measurements (m)</figcaption>
</figure>
</div>
<p>The analysis above is for noiseless, fixed sparsity (k = 3) reconstruction.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/omp-montecarlo-sparsity.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Monte Carlo Trial: Sparsity (k)</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMP-Python/omp-sin/omp-montecarlo-noise.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Monte Carlo Trial: Noise (in dB)</figcaption>
</figure>
</div>
<p>In summary, OMP is a robust and efficient algorithm for compressed sensing when the signal is sparse and the measurement conditions are favorable. However, its sensitivity to noise and the need for sufficient measurements must be considered in practical applications.</p>
</section>
<section id="iterative-shrinkage-thresholding-algorithm-ista" class="level2">
<h2 class="anchored" data-anchor-id="iterative-shrinkage-thresholding-algorithm-ista">3.2 ITERATIVE SHRINKAGE THRESHOLDING ALGORITHM (ISTA)</h2>
<p>The ISTA is an iterative, convex optimisation method for solving sparse signal recovery problems, particularly those formulated as LASSO or basis pursuit denoising. ISTA iteratively updates the solution by applying a gradient descent step followed by a soft-thresholding (shrinkage) operation to promote sparsity. The general steps are:</p>
<ol type="1">
<li>Initialize the sparse coefficient vector.</li>
<li>At each iteration, perform a gradient descent step to minimize the data fidelity term.</li>
<li>Apply the soft-thresholding operator to enforce sparsity.</li>
<li>Repeat until convergence.</li>
</ol>
<p>These steps are as shown, from</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>ISTA Algorithm<span class="citation" data-cites="fast-sparse-coding">[@fast-sparse-coding]</span></figcaption>
</figure>
</div>
<section id="algorithm-implementation-monte-carlo-trial" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-implementation-monte-carlo-trial">3.2.1 Algorithm Implementation &amp; Monte Carlo Trial</h3>
<p>Just like in OMP implemntation, the basic libraries were imported and the sinusoidal input is converted to its sparser domain using DCT. The soft thresholding function plays a role in enforcing sparsity by shrinking very small values to zero. Since convergence is very slow in ISTA, the number of iterations are higher than that of OMP.</p>
<p>Monte Carlo has been implemented in a very similar manner as that of OMP and it has been checked for all the 3 paramters. Moreover, both the algorithms have been compared for these parameters, and their performance has been observed and analysed.</p>
</section>
<section id="observations-results-1" class="level3">
<h3 class="anchored" data-anchor-id="observations-results-1">3.2.2 Observations &amp; Results</h3>
<p>Implementing ISTA for a sinusoidal input had some similarities with that of the OMP algorithm. The trends in the major 3 parameters are same for both the algorithms. Initially ISTA was checked for pure reconstruction, that is no noise interfernce. The result is as plotted,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>ISTA Algorithm Implementation (Ideal)</figcaption>
</figure>
</div>
<p>It is observed that as the number of iterations for ISTA increases, the error decreases.</p>
<p>Now, when noise is added during the process, its performance is also as shown,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>ISTA Algorithm Implementation (With Noise)</figcaption>
</figure>
</div>
<p>Here, unlike OMP, ISTA is very resistant to noise variation and hence explains its advantage over OMP. That is because of the shrinkage function, which shrinks small coefficients to zero and its regularisation term penalises the high variance solutions. In contrary, OMP being a greedy algorithm, tends to select the noisy atoms causing to succumb to the effect of noise. Hence, ISTA is more robust to noise than OMP.</p>
<p>The Monte Carlo for ISTA has been implemented with varying measurements for every value of n.&nbsp;The results are to some extent similar to that of the OMP, that is the reconstruction error follows an inverse relationship with the number of measurements, which is as shown below,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/ISTA-Python/img/istaa-montecarlo.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Monte Carlo Trial: Measurements (m)</figcaption>
</figure>
</div>
<p>With these 3 parameters used for the trial, it can be done to compare both ISTA and OMP. This is done so as to assess and understand the scope of the algorithm for future work, etc. The comparisons are shown below</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMPvsISTA/img/montecarlo-omp-ista-measurements.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP v/s ISTA (m)</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/mainOMPvsISTA/img/montecarlo-omp-ista-noise.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP v/s ISTA (noise)</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/OMPvsISTA/img/montecarlo-omp-ista-sparsity.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>OMP v/s ISTA (sparsity)</figcaption>
</figure>
</div>
<p>In summary, both OMP and ISTA have their own strengths and are suitable for different scenarios in compressed sensing-based radar signal processing:</p>
<ul>
<li><p><strong>OMP</strong> excels when the signal is highly sparse and the number of measurements is sufficient. It is computationally efficient and provides accurate reconstruction in low-noise environments. However, its performance degrades with increased noise or when the sparsity assumption is violated.</p></li>
<li><p><strong>ISTA</strong> is more robust to noise due to its regularization and shrinkage steps. It can handle less sparse signals and noisy measurements better than OMP, albeit at the cost of slower convergence and higher computational complexity. It is best in handling undersampled and noisy data.</p></li>
</ul>
<p>The choice between OMP and ISTA depends on the specific requirements of the application, such as the expected sparsity of the signal, noise levels, and computational resources. In practice, a trade-off must be made between reconstruction accuracy, noise robustness, and computational efficiency.</p>
</section>
</section>
<section id="coordinate-descent-cod" class="level2">
<h2 class="anchored" data-anchor-id="coordinate-descent-cod">3.3 COORDINATE DESCENT (CoD)</h2>
<p>Coordinate Descent is a simple yet powerful optimization algorithm that is widely used in compressed sensing applications, especially for solving large-scale sparse recovery problems such as LASSO (Least Absolute Shrinkage and Selection Operator). It works by minimizing (or maximizing) a function by solving for one variable at a time while keeping the others fixed. The process repeats, cycling through each variable (or “coordinate”) in turn, updating its value to reduce the objective function. The general steps are: 1. Initialize the sparse coefficient vector. 2. For each coordinate (variable), update its value by minimizing the objective function with respect to that coordinate, keeping all other variables fixed. 3. Apply the soft-thresholding operator to the updated coordinate to enforce sparsity. 4. Repeat steps 2 and 3 for all coordinates, cycling through them until convergence.</p>
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/algorithm_cod.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%" alt="CoD Algorithm"> <!-- cod algorithm picture --></p>
<section id="algorithm-implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-implementation-1">3.3.1 Algorithm Implementation</h3>
<p>In the MATLAB implementation of CoD algorithm, The process begins by generating a sparse signal (z_true) and its measurement (y) using a random sensing matrix (Phi). The main loop iteratively updates each coordinate of the sparse coefficient vector (z) by applying the soft-thresholding operator, which enforces sparsity. At each iteration, the coordinate with the largest change is selected and updated to minimize the objective function. The reconstructed signal is then compared to the original, and the mean squared error (MSE) is tracked over iterations to monitor convergence. The code concludes by plotting both the original and reconstructed signals, as well as the MSE progression, illustrating the effectiveness of the CoD algorithm in recovering sparse signals.</p>
</section>
<section id="monte-carlo-trials-1" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-trials-1">3.3.2 Monte Carlo Trials</h3>
<p>A python script was created for the algorithm that closely matches the MATLAB implementation and Monte Carlo Trials were conducted on the algorithm. The input values were stored in a list and these were fed to the trial algorithm. The error was calculated for a number of trials for the same input values, and only the average error is plotted to prevent unwanted variations in reconstruction.</p>
</section>
<section id="observations-results-2" class="level3">
<h3 class="anchored" data-anchor-id="observations-results-2">3.3.3 Observations &amp; Results</h3>
<p>Two types of implementations were considered- an ideal noiseless input signal and an input signal with 10dB of noise. For the ideal condition, multple values of m were considered(32,64 and 128) for n = 128 and the original vs reconstructed signal graphs were plotted.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/ideal/cod_m32.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Ideal CoD at m = 32</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/ideal/cod_m64.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Ideal CoD at m = 64</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/ideal/cod_m128.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Ideal CoD at m = 128</figcaption>
</figure>
</div>
<p>Also, the plot between the Mean Squared Error(MSE) and the number of iterations gives us the convergence of the algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/ideal/cod_conv_ideal.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Iterations vs MSE plot for Ideal CoD</figcaption>
</figure>
</div>
<p>it is observed that increase in resolution, ie. increasing the value of m increases the accuracy of the reconstructed signal. The accuracy is more than enough for m = 32, which is only one fourth of the total samples. at m = 64, the signals is almost entirely reconstructed except for a slight decrease in Amplitude.</p>
<p>Now, for the noisy implementation m = 32 and 64 were considered for n = 128 and the corresponding graphs were plotted.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/noisy/cod_m32_noisy1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Original vs Reconstructed signal: m=32</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/noisy/cod_m32_noisy_mse.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Iterations vs MSE graph: m = 32</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/noisy/cod_m64_noisy1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Original vs Reconstructed signal: m=64</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-MATLAB/img/noisy/cod_m64_noisy_mse.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Iterations vs MSE graph: m = 64</figcaption>
</figure>
</div>
<p>for noisy implementation, n = 32 shows considerable error in amplitudes. Some of the smaller peaks are ignored by the algorithm. Its Iteration vs MSE graph is spiky but still shows a clear trend of convergence. n = 64 shows even better performance. There are still some amplitude errors, but this is very good performance for only half the samples. The Iteration vs MSE graph is much more smooth. Overall, the algorithm is more than capable of reconstructing a noisy signal of low resolution with managable errors. The Monte Carlo for CoD has been implemented in the same way as for OMP and ISTA, with varying measurements for every value of n.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/CoD-Python/img/montecarlo_CoD.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Monte Carlo Trials - CoD</figcaption>
</figure>
</div>
<p>The results show that the reconstruction error is inversely proportional to the number of measurements m and proportional to the length n.</p>
<p>The algorithms discussed above, such as OMP, ISTA, and CoD, generally have computational complexities that scale quadratically or cubically with the problem size. For example, OMP has a worst-case time complexity of cubic terms per signal. ISTA and CoD, while sometimes more efficient per iteration, may require a large number of iterations to converge, leading to overall quadratic or higher complexity.</p>
<p>So, hence, while these algorithms are effective for offline or simulated environments, they are often not sufficient for real-time radar signal processing due to several reasons:</p>
<ul>
<li><p><strong>Computational Complexity:</strong> Algorithms like OMP, ISTA, and CoD can be computationally intensive, especially for large-scale problems or high-dimensional signals. Real-time radar applications require fast processing to meet strict latency requirements, which may not be achievable with these iterative algorithms on standard hardware.</p></li>
<li><p><strong>Latency Constraints:</strong> Real-time systems demand immediate or near-instantaneous responses. The iterative nature of these algorithms can introduce unacceptable delays, making them unsuitable for time-critical radar applications.</p></li>
<li><p><strong>Resource Limitations:</strong> Embedded radar systems often have limited memory and processing power. The memory and computational requirements of these algorithms may exceed the capabilities of such systems.</p></li>
<li><p><strong>Robustness to Dynamic Environments:</strong> Real-time radar must handle rapidly changing environments, interference, and noise. The algorithms discussed may not adapt quickly enough to such variations or may require parameter tuning that is impractical in real time.</p></li>
<li><p><strong>Scalability:</strong> As the number of targets or the dimensionality of the data increases, the performance of these algorithms can degrade, further limiting their applicability in real-time scenarios.</p></li>
</ul>
<p>To address these challenges, we are introducing two modifications in our model,</p>
<ol type="1">
<li><p><strong>Augmented Dictionary for Blind Reconstruction</strong></p></li>
<li><p><strong>Reconstruction Algorithm Unrolling</strong></p></li>
</ol>
</section>
</section>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">5.1 INTRODUCTION</h2>
<p>In traditional compressed sensing, the measurement process assumes prior knowledge of the basis (or dictionary) in which the signal is sparse. However, in many real-world radar applications, the exact basis or the parameters of the signal (such as frequency, phase, or waveform) may not be known in advance. This scenario is referred to as <strong>blind reconstruction</strong>.</p>
<p>Blind reconstruction aims to recover both the sparse signal and the underlying dictionary (or its parameters) from the compressed measurements. One effective approach to address this challenge is to use an <strong>augmented dictionary</strong>— a collection of candidate basis vectors that span a wider range of possible signal structures.</p>
<p>So, the upcoming sections will discuss the implementation and the observations drawn from it.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">5.2 IMPLEMENTATION</h2>
<p>In the previous implementations, the input signal type is known and hence, the basis is also known and the compression is done with ease. But in real life, the the radar can receive any type of signals, and some of them might not be sparse enough for the signal to be taken for reconstruction.</p>
<p>For now, only 3 types of signals are accepted as inputs, that is <strong>Single tone Sinusoids</strong>, <strong>LFM(Linear Frequency Modulated)</strong> a.k.a <strong>Chirp signals</strong> and <strong>BPSK (Binary Phase Shifted Key)</strong> signals. The input signals are generated and is sent through a mixer. The suitable basis is analysed and selected for making the augmented basis by concatenating each of the individual dictionaries.</p>
<p><span class="math display">\[\begin{equation}
\boxed{
    \Psi_{\text{aug}} = \left[\, \Psi_{\text{sinusoid}} \;\Big|\; \Psi_{\text{LFM}} \;\Big|\; \Psi_{\text{BPSK}} \,\right]
}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\Psi_{\text{sinusoid}}\)</span>, <span class="math inline">\(\Psi_{\text{LFM}}\)</span>, and <span class="math inline">\(\Psi_{\text{BPSK}}\)</span> are the basis matrices for Single-tone sinusoids, LFM signals, and BPSK signals, respectively, and <span class="math inline">\(\Psi_{\text{aug}}\)</span> is the augmented dictionary formed by concatenating these bases.</p>
<p>A gaussian noise is also added to check its performance in noisy environments.</p>
</section>
<section id="observations-results-3" class="level2">
<h2 class="anchored" data-anchor-id="observations-results-3">5.3 OBSERVATIONS &amp; RESULTS</h2>
<p>Initially the DCT matrix is used as the basis for the above mentioned input signals and their results are as shown,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/Aug-Dict/img/input-dct.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>DCT on different radar signals</figcaption>
</figure>
</div>
<p>As shown above, other than the single tone sinusoids, the other two matrices are not as sparse as expected. Even though, chirp signals are expected to be not sparse, the BPSK signal is not sparse enough for reconstruction.</p>
<p>So, for easy implementation of the dictionaries, the respective atoms are created and is used as a dictionaries, that is, the bpsk and chirp signal values are generated and filled to their respective matrix as their dictionaries.</p>
<p>The mixer initially just added the 3 signals, but later on coefficients were added for better understanding of its sparsity. The results, however, are not really as satisfactory as expected. The signal reconstruction is as shown</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/Aug-Dict/img/n128-m64-ideal.png" class="img-fluid figure-img"></p>
<figcaption>Augmented Dictionary Implementation (Ideal): n=128 m=64</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/Aug-Dict/img/n128-m64-noisy.png" class="img-fluid figure-img"></p>
<figcaption>Augmented Dictionary Implementation (Noisy): n=128 m=64</figcaption>
</figure>
</div>
<p>so, now instead of just simply adding the signals as it is, coefficients were added (a, b, c) such that each component of the input signal can be tuned for observation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/Aug-Dict/img/n128-m64-ideal-mixer.png" class="img-fluid figure-img"></p>
<figcaption>Augmented Dictionary Implementation (Ideal) with mixer: n=128 m=64</figcaption>
</figure>
</div>
<p>It is observed that the reconstruction error is pretty significant compared to the overall amplitude of the input signal.</p>
<p>From the above implementation, we can say that the augmeneted dictionary is a good attempt in assessing multiple radar input signals. But despite that, the dictionary still needs to be pre-defined.</p>
<p>Moreover, most of the other radar signals are non-linear and some of these signals can’t have a basis for sparsity.</p>
<p>The best option is to adopt a learned method, or autoencoder to learn a non-linear based basis for all the possible trained signals.</p>
</section>
<section id="introduction-3" class="level2">
<h2 class="anchored" data-anchor-id="introduction-3">6.1 INTRODUCTION</h2>
<p>As explained in the sections above, the current reconstruction algorithms are iterative, hence require a long computational period before an output is generated, which hampers the real time processing of signal. One way to avoid this is improving the Reconstruction algorithms to work real time.For such a case, machine learning techniques could help. One promising approach is <strong>algorithm unrolling</strong> (also known as unfolding), where the iterative steps of a reconstruction algorithm are mapped onto the layers of a neural network<span class="citation" data-cites="fast-sparse-coding">[@fast-sparse-coding]</span>. This technique enables the integration of domain knowledge from classical algorithms with the learning capacity of neural networks, resulting in models that are both interpretable and highly effective.</p>
<p>Algorithm unrolling involves representing each iteration of an algorithm—such as ISTA or OMP—as a layer in a neural network. The parameters of these layers (e.g., thresholds, step sizes, or transforms) can then be learned from data, allowing the network to adapt to the specific characteristics of the signals and measurement process. This approach not only accelerates convergence compared to traditional iterative methods but also improves reconstruction quality, especially in challenging scenarios with noise or model mismatch.</p>
<p>In this section, we discuss its advantages for real-time radar signal processing, and introduce two unrolled algorithms - <strong>Learned ISTA(LISTA)</strong> and <strong>Learned CoD(LCoD)</strong>.</p>
</section>
<section id="advantages" class="level2">
<h2 class="anchored" data-anchor-id="advantages">6.2 ADVANTAGES</h2>
<p>Algorithm unfolding is a technique that transforms iterative algorithms (like those used in optimization or signal processing) into a sequence of layers, similar to neural networks. This approach offers several advantages:</p>
<p><strong>Interpretability</strong>: Each layer corresponds to a step in the original algorithm, making the model’s operations easier to understand and analyze. <strong>Efficiency</strong>: By learning parameters for each layer, unfolded algorithms can converge faster and require fewer iterations than traditional methods. <strong>Adaptability</strong>: The unfolded structure can be trained end-to-end, allowing it to adapt to specific data distributions or tasks. <strong>Performance</strong>: Often achieves better accuracy and robustness compared to standard iterative algorithms, especially when combined with data-driven learning.</p>
</section>
<section id="learned-ista-lista" class="level2">
<h2 class="anchored" data-anchor-id="learned-ista-lista">6.3 LEARNED ISTA (LISTA)</h2>
<p>Learned ISTA (LISTA) is a neural network-based approach that unrolls the traditional ISTA algorithm into a fixed number of layers, where each layer mimics one iteration of ISTA. Unlike standard ISTA, where parameters such as step size and thresholds are fixed, LISTA learns these parameters from data during training. This enables faster convergence and improved reconstruction accuracy. Each layer of the LISTA network consists of a linear transformation followed by a non-linear shrinkage (soft-thresholding) operation, and the parameters of these operations are optimized by successive iterations. As a result, LISTA combines the interpretability of classical algorithms with the adaptability and efficiency of deep learning, making it well-suited for real-time and large-scale compressed sensing applications.</p>
<section id="algorithm-implementation-2" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-implementation-2">6.3.1 Algorithm Implementation</h3>
<p>The algorithm implemented here has been improvised from the Python implementation of LISTA from <span class="citation" data-cites="shlezinger_modelbaseddeeplearning">[@shlezinger_modelbaseddeeplearning]</span>. All the required packages were imported and the seed was fixed for reproducibility. A simulated dataset was generated with a random dictionary and sparse signal. This was plotted.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/sample_seed.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Plot of Simulated Data</figcaption>
</figure>
</div>
<p>First the ISTA algorithm was applied to the test set. The untrained LISTA algorithm was then applied to the dataset, and the results were compared with the standard ISTA algorithm. Now, the learned LISTA algorithm was applied to the dataset, and the results were compared with the standard ISTA algorithm.</p>
</section>
<section id="observations-and-conclusion" class="level3">
<h3 class="anchored" data-anchor-id="observations-and-conclusion">6.3.2 Observations and Conclusion</h3>
<p>The Iterations vs MSE plot of ISTA and Untrained LISTA shows clear convergence. The untrained LISTA and classic ISTA algorithms should be mathematically equivalent,but the starting point of both algorithms are different, indicating issues with equivalence. But both algorithms converge to the same reconstruction error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/ista_vs_untrained_lista.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>ISTA vs Untrained LISTA</figcaption>
</figure>
</div>
<p>The iterations vs MSE plot of ISTA vs Trained LISTA is given below. The trained LISTA algorithm converges to a better reconstruction error than the untrained LISTA algorithm, using very low numbers of iterations. The starting point of both algorithms is different, indicating the same issues with equivalence as that of the above plot.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/ista_vs_trained_lista.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>ISTA vs trained LISTA</figcaption>
</figure>
</div>
<p>The conclusion is that Learned ISTA (LISTA) method is more than capable of replacing ISTA, combining the interpretability of classic ISTA with the adaptability and efficiency of neural networks.</p>
</section>
</section>
<section id="learned-coordinate-descent-lcod" class="level2">
<h2 class="anchored" data-anchor-id="learned-coordinate-descent-lcod">6.4 LEARNED COORDINATE DESCENT (LCoD)</h2>
<p>The Learned Coordinate Descent (LCoD) algorithm is an optimization technique that iteratively updates one coordinate (variable) at a time to minimize a target function. Unlike standard coordinate descent, LCoD leverages machine learning to adaptively select which coordinate to update and how much to change it, based on patterns learned from data. This approach can lead to faster convergence and improved performance, especially in high-dimensional or structured problems.</p>
<section id="algorithm-implementation-3" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-implementation-3">6.4.1 Algorithm Implementation</h3>
<p>The code follows the same algorithmic flow as the previous Python implementation of LISTA. All the required packages were imported and the seed was fixed for reproducibility. A simulated dataset was generated with a random dictionary and sparse signal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/sample_seed_cod.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Plot of Simulated Data</figcaption>
</figure>
</div>
<p>First the CoD algorithm was applied to the test set. The untrained LCoD algorithm was then applied to the dataset, and the results were compared with the standard CoD algorithm. Now, the learned LCoD algorithm was applied to the dataset, and the results were compared with the standard CoD algorithm.</p>
</section>
<section id="observations" class="level3">
<h3 class="anchored" data-anchor-id="observations">6.4.2 Observations</h3>
<p>The Iterations vs MSE plot of CoD and Untrained LCoD shows that both algorithms are mathematically equivalent. This is illustated in the plot below. This is so, since no optimisations or changes were made to the standard iterative step of CoD to create unrolled iterative step of LCoD.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/CoDvsUntrainedCoD.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>COD vs Untrained LCoD</figcaption>
</figure>
</div>
<p>The iterations vs MSE plot of CoD vs Trained lCoD is given below. The trained LISTA algorithm converges to a better reconstruction error than the untrained LISTA algorithm, using lesser numbers of iterations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/CoDvsTrainedCoD.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>COD vs trained LCoD</figcaption>
</figure>
</div>
<p>A similar conclusion is made to the previous section.</p>
</section>
</section>
<section id="comparing-both-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="comparing-both-algorithms">6.5 COMPARING BOTH ALGORITHMS</h2>
<p>A Python implementation comparing the performance of Learned ISTA (LISTA) and Learned CoD (LCoD) algorithms on a simulated dataset is provided.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ninja-boy/CS-Algorithms/main/LISTAandLCOD/img/lista_vs_lcod.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Comparison of LISTA and LCoD</figcaption>
</figure>
</div>
<p>The plot above compares the MSE vs Iterations graph of Learned ISTA (LISTA) and Learned CoD (LCoD) algorithms. Both algorithms converge to the same reconstruction error. However, two major observations were made:</p>
<ol type="1">
<li>The iterations of LISTA is more inefficient compared to LCoD.</li>
</ol>
<p>This is because CoD is an iterative algorithm that selects and updates one coordinate at a time greedily, hence it is guaranteed that there is a change in MSE per iteration. ISTA does not have such guarantees, hence LISTA may have higher MSE than LCoD at the same number of iterations.</p>
<ol start="2" type="1">
<li>LCoD have higher computation time per iteration compared to LISTA.</li>
</ol>
<p>As explained above, CoD selects and updates coordinates one at a time per iteration, hence it is more computationally expensive. Also, The iterative step of CoD in the current implementation is not optimised for torch operations, therefore steps involving torch operations take longer to compute. The algorithm is serial by definition, thus training involving batch operations needs workarounds to make it happen. Hence, the current implementation takes longer time to compute than ISTA or LISTA for the given problem.</p>
<p>The faster computation time of LISTA is beneficial for real-time signal processing applications involving small-scale and parallelised computing. But for larger scale LASSO problems, LCoD can be more efficient, since the convergence guarentee of CoD means even though individual iterations can be time-consuming, the number of iterations required is much lower, which means the total computation time compared to LISTA is less.</p>
</section>
<section id="final-remarks" class="level2">
<h2 class="anchored" data-anchor-id="final-remarks">6.6 FINAL REMARKS</h2>
<p>The above observations show that Algorithm unrolling can significantly improve the performance of iterative algorithms for a fraction of the number of iterations. This method combines low sampling requirement of compressed sensing algorithms with lower time complexity of learned ML models, both of which are important requirements for real-time signal processing applications.</p>
<p>This report lays a solid foundation for advanced research and practical applications in compresses sensing for radar signal processing.</p>
<ol type="1">
<li><p>In this project, the input signals are purely real, hence the use of DCT as basis. But in real life applications, the phase (complex) part is also applicable.</p></li>
<li><p>We have explored different reconstruction algorithms (like OMP, ISTA, CoD). The reconstruction can be further enhanced by other advanced algorithms like Fast ISTA (FISTA) or Alternating Direction Method of Multipliers (ADMM).</p></li>
<li><p>Instead of augmented dictionary, a learned dictionary (Autoencoder) can be used for predicting the non-linear radar signals.</p></li>
<li><p>Optimise torch operations for LCoD.</p></li>
<li><p>Trying to implement the real time processing into a hardware (FPGAs/Embedded GPUs) by reducing model size and complexity for low latency power.</p></li>
</ol>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" role="list">

</div>
<p><strong>FUNCTION:-</strong></p>
<ul>
<li><strong>Sine Wave Generator</strong></li>
</ul>
<div id="fd54dbf5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_sine_signal(n, k, freq<span class="op">=</span><span class="dv">5</span>, fs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.arange(n) <span class="op">/</span> fs</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    sum_signal <span class="op">=</span> np.zeros(n)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        freq <span class="op">=</span> i <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        sum_signal <span class="op">+=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> freq <span class="op">*</span> t)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sum_signal</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Noise Generator</strong></li>
</ul>
<div id="842315a8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_noise(y, snr_db):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    signal_power <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(y)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    snr_linear <span class="op">=</span> <span class="dv">10</span><span class="op">**</span>(snr_db <span class="op">/</span> <span class="dv">10</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    noise_power <span class="op">=</span> signal_power <span class="op">/</span> snr_linear</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> np.sqrt(noise_power) <span class="op">*</span> np.random.randn(<span class="op">*</span>y.shape)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y <span class="op">+</span> noise</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>OMP Function (Python)</strong></li>
</ul>
<div id="253131ca" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> omp(y, A, tol<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> A.shape</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> y.copy()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    idx_set <span class="op">=</span> []</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    x_hat <span class="op">=</span> np.zeros(n)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        correlations <span class="op">=</span> A.T <span class="op">@</span> r</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.argmax(np.<span class="bu">abs</span>(correlations))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        idx_set.append(idx)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        A_selected <span class="op">=</span> A[:, idx_set]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        x_ls, _, _, _ <span class="op">=</span> np.linalg.lstsq(A_selected, y, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> y <span class="op">-</span> A_selected <span class="op">@</span> x_ls</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(r) <span class="op">&lt;</span> tol:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x_hat[idx_set] <span class="op">=</span> x_ls</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>OMP Function (MATLAB)</strong></li>
</ul>
<pre class="{matlab}"><code>clc; close all; clear all;
function x = omp(A, b, K)
    originalA = A;               % Store the original A
    norms = vecnorm(A);
    A = A ./ norms;
    r = b;
    Lambda = [];
    N = size(A, 2);
    x = zeros(N, 1);

    for k = 1:K
        h_k = abs(A' * r);
        h_k(Lambda) = 0;
        [~, l_k] = max(h_k);

        Lambda = [Lambda, l_k];
        Asub = A(:, Lambda);
        x_sub = Asub \ b;

        x = zeros(N, 1);
        x(Lambda) = x_sub ./ norms(Lambda)';
        r = b - originalA(:, Lambda) * x(Lambda);   % Corrected
    end
end</code></pre>
<ul>
<li><strong>Monte-Carlo Trial</strong></li>
</ul>
<div id="8832a85f" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.fftpack <span class="im">import</span> dct, idct</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> monte_carlo_trial(n, m, sampling_rate):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    x_time <span class="op">=</span> generate_sine_signal(n, sampling_rate)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    x_sparse <span class="op">=</span> dct(x_time, norm<span class="op">=</span><span class="st">'ortho'</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> measurement(m, n)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> A <span class="op">@</span> x_sparse</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    x_sparse_rec <span class="op">=</span> omp(y, A)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    x_time_rec <span class="op">=</span> idct(x_sparse_rec, norm<span class="op">=</span><span class="st">'ortho'</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> norm(x_time <span class="op">-</span> x_time_rec, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> error</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Monte Carlo Simulation and Plotting ----</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>num_trials <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>m_values <span class="op">=</span> np.arange(<span class="dv">2</span>, <span class="dv">65</span>, <span class="dv">2</span>)  <span class="co"># Number of measurements</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>noise_val <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">51</span>, <span class="dv">5</span>)  <span class="co"># Noise levels in dB</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">11</span>, <span class="dv">1</span>)  <span class="co"># Sparsity levels</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>sampling_rate <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> n_values:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    avg_errors <span class="op">=</span> []</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> m <span class="kw">in</span> m_values:</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> m <span class="op">&gt;=</span> n:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            avg_errors.append(np.nan)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        errors <span class="op">=</span> []</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_trials):</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>            errors.append(monte_carlo_trial(n, m, sampling_rate))</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        avg_errors.append(np.mean(errors))</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    plt.plot(m_values, avg_errors, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="ss">f'n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>ISTA Function (Python)</strong></li>
</ul>
<div id="2d70d431" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> soft_thresholding(x, threshold):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Soft thresholding operator h_{alpha/L}"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sign(x) <span class="op">*</span> np.maximum(np.<span class="bu">abs</span>(x) <span class="op">-</span> threshold, <span class="fl">0.0</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ista(X, W_d, max_iter<span class="op">=</span><span class="dv">200</span>, tol<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="fl">1.1</span> <span class="op">*</span> np.linalg.norm(W_d.T <span class="op">@</span> W_d, <span class="dv">2</span>)  </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># L &gt; max eigenvalue of W_d.T @ W_d</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> W_d.shape</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.zeros(n)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        Z_old <span class="op">=</span> Z.copy()</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> W_d.T <span class="op">@</span> (W_d <span class="op">@</span> Z <span class="op">-</span> X)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">-=</span> gradient <span class="op">/</span> L</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">=</span> soft_thresholding(Z <span class="op">-</span> (<span class="fl">1.0</span> <span class="op">/</span> L) <span class="op">*</span> gradient, alpha <span class="op">/</span> L)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(Z <span class="op">-</span> Z_old, <span class="bu">ord</span> <span class="op">=</span> <span class="dv">2</span>) <span class="op">&lt;</span> tol:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>CoD Implementation (Python)</strong></li>
</ul>
<div id="e00aed72" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Coordinate Descent Algorithm</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.zeros((n, <span class="dv">1</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> theta.T <span class="op">@</span> y  <span class="co"># Initial coefficients</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.eye(n) <span class="op">-</span> theta.T <span class="op">@</span> theta  <span class="co"># Residual matrix</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>mse_history <span class="op">=</span> np.zeros(num_iter)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(num_iter):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    z_bar <span class="op">=</span> np.sign(B) <span class="op">*</span> np.maximum(np.<span class="bu">abs</span>(B) <span class="op">-</span> alpha, <span class="dv">0</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> np.argmax(np.<span class="bu">abs</span>(z <span class="op">-</span> z_bar))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> z_bar[k, <span class="dv">0</span>] <span class="op">-</span> z[k, <span class="dv">0</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> B <span class="op">+</span> S[:, [k]] <span class="op">*</span> delta</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    z[k, <span class="dv">0</span>] <span class="op">=</span> z_bar[k, <span class="dv">0</span>]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    x_rec_iter <span class="op">=</span> Psi <span class="op">@</span> z</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    mse_history[t] <span class="op">=</span> np.mean((x <span class="op">-</span> x_rec_iter) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>x_rec <span class="op">=</span> Psi <span class="op">@</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>RADAR signal Generator</strong></li>
</ul>
<div id="50e825d9" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.signal <span class="im">import</span> chirp</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_sine_signals(batch_size, length, freq_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">10</span>)):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, length)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    signals <span class="op">=</span> []</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        freq <span class="op">=</span> np.random.uniform(<span class="op">*</span>freq_range)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        phase <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        signal <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> freq <span class="op">*</span> t <span class="op">+</span> phase)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        signals.append(signal)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(signals, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_bpsk_signals(batch_size, length):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    symbols <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> torch.randint(<span class="dv">0</span>, <span class="dv">2</span>, (batch_size, length)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> symbols.<span class="bu">float</span>()</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_chirp_signals(batch_size, length, f0<span class="op">=</span><span class="dv">5</span>, f1<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, length)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    signals <span class="op">=</span> []</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        signal <span class="op">=</span> chirp(t, f0<span class="op">=</span>f0, f1<span class="op">=</span>f1, t1<span class="op">=</span><span class="dv">1</span>, method<span class="op">=</span><span class="st">'linear'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        signals.append(signal)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(signals, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>sine_signals <span class="op">=</span> generate_sine_signals(batch_size, length)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>bpsk_signals <span class="op">=</span> generate_bpsk_signals(batch_size, length)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>chirp_signals <span class="op">=</span> generate_chirp_signals(batch_size, length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Augmented Dictionary</strong></li>
</ul>
<div id="335b2623" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.fftpack <span class="im">import</span> dct, idct</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> soft_thresholding(x, threshold):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Soft thresholding operator h_{alpha/L}"""</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sign(x) <span class="op">*</span> np.maximum(np.<span class="bu">abs</span>(x) <span class="op">-</span> threshold, <span class="fl">0.0</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ista(X, W_d, max_iter<span class="op">=</span><span class="dv">200</span>, tol<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="fl">1.1</span> <span class="op">*</span> np.linalg.norm(W_d.T <span class="op">@</span> W_d, <span class="dv">2</span>)  </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># L &gt; max eigenvalue of W_d.T @ W_d</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> W_d.shape</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.zeros(n)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        Z_old <span class="op">=</span> Z.copy()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> W_d.T <span class="op">@</span> (W_d <span class="op">@</span> Z <span class="op">-</span> X)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">-=</span> gradient <span class="op">/</span> L</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">=</span> soft_thresholding(Z <span class="op">-</span> (<span class="fl">1.0</span> <span class="op">/</span> L) <span class="op">*</span> gradient, alpha <span class="op">/</span> L)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(Z <span class="op">-</span> Z_old, <span class="bu">ord</span> <span class="op">=</span> <span class="dv">2</span>) <span class="op">&lt;</span> tol:</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Signal Setup</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="bu">input</span>(<span class="st">"Enter length of signal (n): "</span>))  <span class="co"># Signal length</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Sinusoidal component (tone)</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>tone <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> <span class="dv">10</span> <span class="op">*</span> t)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co"># BPSK component</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>bits <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">2</span>, n)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>bpsk <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> bits <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> t)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Chirp component</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>f0, k <span class="op">=</span> <span class="dv">5</span>, <span class="dv">80</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>chirp <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> (f0 <span class="op">*</span> t <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> k <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Mixed signal </span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>a, b, c <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>  <span class="co"># Coefficients for mixing</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>signal <span class="op">=</span> a<span class="op">*</span>tone <span class="op">+</span> b<span class="op">*</span>bpsk <span class="op">+</span> c<span class="op">*</span>chirp</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Sub-dictionary 1: DCT</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>D_dct <span class="op">=</span> np.eye(n)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>D_dct <span class="op">=</span> idct(D_dct, norm<span class="op">=</span><span class="st">'ortho'</span>).T</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Sub-dictionary 2: Chirp-like atoms</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_chirp_atoms(n, num_atoms):</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    D_chirp <span class="op">=</span> np.zeros((n, num_atoms))</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_atoms):</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        f0 <span class="op">=</span> np.random.uniform(<span class="dv">5</span>, <span class="dv">20</span>)</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> np.random.uniform(<span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        D_chirp[:, i] <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> (f0 <span class="op">*</span> t <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> k <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>    D_chirp <span class="op">/=</span> np.linalg.norm(D_chirp, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D_chirp</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>D_chirp <span class="op">=</span> generate_chirp_atoms(n, n)</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Sub-dictionary 3: BPSK atoms</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_bpsk_atoms(n, num_atoms, fc<span class="op">=</span><span class="dv">5</span>, fs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> np.arange(n) <span class="op">/</span> fs</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>    D_bpsk <span class="op">=</span> np.zeros((n, num_atoms))</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_atoms):</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>        bits <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">2</span>, n)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        symbols <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> bits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>        D_bpsk[:, i] <span class="op">=</span> symbols <span class="op">*</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> fc <span class="op">*</span> t)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize atoms</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    D_bpsk <span class="op">/=</span> np.linalg.norm(D_bpsk, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D_bpsk</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage:</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>D_bpsk <span class="op">=</span> generate_bpsk_atoms(n, n)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>D_total <span class="op">=</span> np.concatenate([D_dct, D_dct, D_chirp], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Compressed Measurement</span></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">int</span>(<span class="bu">input</span>(<span class="st">"Enter number of compressed samples (m &lt; n): "</span>))  <span class="co"># Compressed samples</span></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> np.random.randn(m, n)<span class="co"># / np.sqrt(k)  # Measurement matrix</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> Phi <span class="op">@</span> signal</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Add noise to the measurements</span></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>snr_db <span class="op">=</span> <span class="bu">int</span>(<span class="bu">input</span>(<span class="st">"Enter noise to be added (dB): "</span>))  <span class="co"># Signal-to-noise ratio in dB</span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>y_noisy <span class="op">=</span> add_noise(y, snr_db)</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct sensing matrix (Theta)</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> Phi <span class="op">@</span> D_total</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Sparse Recovery using ISTA</span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a><span class="co">#z = ista(y_noisy, A)</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> omp(y, A)</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>signal_hat <span class="op">=</span> D_total <span class="op">@</span> z</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate reconstruction error</span></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> np.linalg.norm(signal <span class="op">-</span> signal_hat)</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reconstruction error (L2 norm): </span><span class="sc">{</span>error<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a><span class="co"># 9. Plot original and reconstructed</span></span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>plt.plot(t, signal, label<span class="op">=</span><span class="st">"Original Mixed Signal"</span>)</span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>plt.plot(t, signal_hat, <span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Reconstructed Signal (ISTA)"</span>)</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mixed Signal Reconstruction from Compressed Measurements"</span>)</span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Time"</span>)</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Amplitude"</span>)</span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Creating Simulated Dataset</strong></li>
</ul>
<div id="aa8ea177" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimulatedData(Data.Dataset): <span class="co">#Creates tuple (x, H, s) for each sample</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, x, H, s):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> x</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.s <span class="op">=</span> s</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> H</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x.shape[<span class="dv">0</span>]</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.x[idx, :]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.H</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.s[idx, :]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, H, s</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data_set(H, n, m, k, N<span class="op">=</span><span class="dv">1000</span>, batch_size<span class="op">=</span><span class="dv">512</span>, signal_dev<span class="op">=</span><span class="fl">0.5</span>, noise_dev<span class="op">=</span><span class="fl">0.01</span>): <span class="co">#function to create dataset</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialization</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.zeros(N, n)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> torch.zeros(N, m)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create signals</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a k-sparsed signal s</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        index_k <span class="op">=</span> np.random.choice(m, k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        peaks <span class="op">=</span> signal_dev <span class="op">*</span> np.random.randn(k)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        s[i, index_k] <span class="op">=</span> torch.from_numpy(peaks).to(s)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X = Hs+w</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        x[i, :] <span class="op">=</span> H <span class="op">@</span> s[i, :] <span class="op">+</span> noise_dev <span class="op">*</span> torch.randn(n)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    simulated <span class="op">=</span> SimulatedData(x<span class="op">=</span>x, H<span class="op">=</span>H, s<span class="op">=</span>s)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    data_loader <span class="op">=</span> Data.DataLoader(dataset<span class="op">=</span>simulated, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data_loader</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000</span> <span class="co"># number of samples</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">150</span> <span class="co"># dim(x)</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">200</span> <span class="co"># dim(s)</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">4</span> <span class="co"># k-sparse signal</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>T_t <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Number of iterations</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Measurement matrix</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> torch.randn(n, m)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>H <span class="op">/=</span> torch.norm(H, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate datasets</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> create_data_set(H, n<span class="op">=</span>n, m<span class="op">=</span>m, k<span class="op">=</span>k, N<span class="op">=</span>N)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> create_data_set(H, n<span class="op">=</span>n, m<span class="op">=</span>m, k<span class="op">=</span>k, N<span class="op">=</span>N, batch_size<span class="op">=</span>N)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>LISTA Class</strong></li>
</ul>
<div id="f481d3b5" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LISTA_Model(nn.Module):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n, m, L, T<span class="op">=</span><span class="dv">6</span>, rho<span class="op">=</span><span class="fl">1.0</span>, H<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LISTA_Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n, <span class="va">self</span>.m <span class="op">=</span> n, m</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> H</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> L</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T  <span class="co"># ISTA Iterations</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rho <span class="op">=</span> rho  <span class="co"># Lagrangian Multiplier</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> nn.Linear(n, m, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># Weight Matrix</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.B <span class="op">=</span> nn.Linear(m, m, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># Weight Matrix</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ISTA Stepsizes eta</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.ones(T <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)<span class="op">/</span>L, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> nn.Parameter(torch.ones(T <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)<span class="op">/</span>L, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialization</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> H <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.A.weight.data <span class="op">=</span> H.t()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.B.weight.data <span class="op">=</span> H.t() <span class="op">@</span> H</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''for param in self.A.parameters(): # A needs no_grad (should not be trained)</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">            param.requires_grad = False</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">        for param in self.B.parameters():# B needs no_grad (should not be trained)</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">            param.requires_grad = False'''</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _shrink(<span class="va">self</span>, s, beta):</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> beta <span class="op">*</span> F.softshrink(s <span class="op">/</span> beta, lambd<span class="op">=</span><span class="va">self</span>.rho)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, s_gt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        mse_vs_itr <span class="op">=</span> []</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        s_hat <span class="op">=</span> <span class="va">self</span>._shrink(<span class="va">self</span>.mu[<span class="dv">0</span>, :, :] <span class="op">*</span> <span class="va">self</span>.A(x), <span class="va">self</span>.beta[<span class="dv">0</span>, :, :])</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="va">self</span>.T <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>            s_hat <span class="op">=</span> <span class="va">self</span>._shrink(s_hat <span class="op">-</span> <span class="va">self</span>.mu[i, :, :] <span class="op">*</span> <span class="va">self</span>.B(s_hat) <span class="op">+</span> <span class="va">self</span>.mu[i, :, :] <span class="op">*</span> <span class="va">self</span>.A(x),</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>                                 <span class="va">self</span>.beta[i, :, :], )</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aggregate each iteration's MSE loss</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>                mse_vs_itr.append(F.mse_loss(s_hat.detach(), s_gt.detach(), reduction<span class="op">=</span><span class="st">"sum"</span>).data.item())</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> s_hat, mse_vs_itr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>LCoD Class</strong></li>
</ul>
<div id="b2f6275e" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LearnedCoD(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, H, T<span class="op">=</span><span class="dv">10</span>, learn_alpha<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LearnedCoD, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> H</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m <span class="op">=</span> H.shape[<span class="dv">1</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed matrices (you can make them learnable if needed)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.B_mat <span class="op">=</span> nn.Parameter(H.T.clone(), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S_mat <span class="op">=</span> nn.Parameter(torch.eye(<span class="va">self</span>.m, dtype<span class="op">=</span>torch.float64) <span class="op">-</span> H.T <span class="op">@</span> H, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learnable thresholds</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> learn_alpha:</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.alpha_list <span class="op">=</span> nn.ParameterList([nn.Parameter(torch.tensor(<span class="fl">0.05</span>, dtype<span class="op">=</span>torch.float64)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(T)])</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.alpha_list <span class="op">=</span> [torch.tensor(<span class="fl">0.05</span>, dtype<span class="op">=</span>torch.float64) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(T)]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> soft_threshold(<span class="va">self</span>, B, alpha):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sign(B) <span class="op">*</span> torch.maximum(torch.<span class="bu">abs</span>(B) <span class="op">-</span> alpha, torch.zeros_like(B))</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, s_gt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x.dim() <span class="op">==</span> <span class="dv">2</span> <span class="kw">and</span> x.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.H.shape[<span class="dv">0</span>]:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>            z_out <span class="op">=</span> []</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>            mse_list <span class="op">=</span> []</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>                z_i, mse_i <span class="op">=</span> <span class="va">self</span>.forward(x[i].unsqueeze(<span class="dv">1</span>), s_gt[i].unsqueeze(<span class="dv">1</span>) <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">None</span>)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>                z_out.append(z_i.squeeze(<span class="dv">1</span>))  <span class="co"># (m, 1) -&gt; (m,)</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>                    mse_list.append(mse_i)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>            z_out <span class="op">=</span> torch.stack(z_out, dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># shape: (batch_size, m)</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>                mse_avg <span class="op">=</span> torch.stack(mse_list, dim<span class="op">=</span><span class="dv">0</span>).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> z_out, mse_avg</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> z_out, <span class="va">None</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Single sample mode (x shape = [n, 1])</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        B <span class="op">=</span> <span class="va">self</span>.B_mat <span class="op">@</span> x  <span class="co"># shape: (m, 1)</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.zeros((<span class="va">self</span>.m, <span class="dv">1</span>), dtype<span class="op">=</span>torch.float64, device<span class="op">=</span>x.device)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        mse_vs_iter <span class="op">=</span> []</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.T):</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>            z_bar <span class="op">=</span> <span class="va">self</span>.soft_threshold(B, <span class="va">self</span>.alpha_list[t])</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> torch.argmax(torch.<span class="bu">abs</span>(z <span class="op">-</span> z_bar))</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>            delta <span class="op">=</span> z_bar[k, <span class="dv">0</span>] <span class="op">-</span> z[k, <span class="dv">0</span>]</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>            B <span class="op">=</span> B <span class="op">+</span> <span class="va">self</span>.S_mat[:, [k]] <span class="op">*</span> delta</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>            z[k, <span class="dv">0</span>] <span class="op">=</span> z_bar[k, <span class="dv">0</span>]</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>                mse <span class="op">=</span> F.mse_loss(z.detach(), s_gt.detach(), reduction<span class="op">=</span><span class="st">"sum"</span>).item()</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>                mse_vs_iter.append(mse)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s_gt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z, torch.tensor(mse_vs_iter, dtype<span class="op">=</span>torch.float64)</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z, <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Training Function</strong></li>
</ul>
<p>def train(model, train_loader, valid_loader, num_epochs=50): # Initialization optimizer = torch.optim.SGD( model.parameters(), lr=5e-05, momentum=0.9, weight_decay=0, ) scheduler = torch.optim.lr_scheduler.StepLR( optimizer, step_size=50, gamma=0.1 ) loss_train = np.zeros((num_epochs,)) loss_test = np.zeros((num_epochs,)) # Main loop for epoch in range(num_epochs): model.train() train_loss = 0 for step, (b_x, b_H, b_s) in enumerate(train_loader): s_hat, _ = model.forward(b_x) loss = F.mse_loss(s_hat, b_s, reduction=“sum”) optimizer.zero_grad() loss.backward() optimizer.step() model.zero_grad() train_loss += loss.data.item() loss_train[epoch] = train_loss / len(train_loader.dataset) scheduler.step()</p>
<pre><code>    # validation
    model.eval()
    test_loss = 0
    for step, (b_x, b_H, b_s) in enumerate(valid_loader):
        # b_x, b_H, b_x = b_x.cuda(), b_H.cuda(), b_s.cuda()
        s_hat, _ = model.forward(b_x)
        test_loss += F.mse_loss(s_hat, b_s, reduction="sum").data.item()
    loss_test[epoch] = test_loss / len(valid_loader.dataset)
    # Print
    if epoch % 10 == 0:
        print(
            "Epoch %d, Train loss %.8f, Validation loss %.8f"
            % (epoch, loss_train[epoch], loss_test[epoch])
        )

return loss_test
```</code></pre>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>